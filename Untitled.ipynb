{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "cbe44e88-cd23-4778-b7c5-7e7b31f93dc3",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import torch\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "import pytorch3d\n",
    "\n",
    "\n",
    "from pytorch3d.utils import ico_sphere\n",
    "import numpy as np\n",
    "from tqdm.notebook import tqdm\n",
    "\n",
    "# Util function for loading meshes\n",
    "from pytorch3d.io import load_objs_as_meshes, save_obj\n",
    "\n",
    "from pytorch3d.loss import (\n",
    "    chamfer_distance, \n",
    "    mesh_edge_loss, \n",
    "    mesh_laplacian_smoothing, \n",
    "    mesh_normal_consistency,\n",
    ")\n",
    "\n",
    "# Data structures and functions for rendering\n",
    "from pytorch3d.structures import Meshes\n",
    "from pytorch3d.renderer import (\n",
    "    look_at_view_transform,\n",
    "    OpenGLPerspectiveCameras, \n",
    "    PointLights, \n",
    "    DirectionalLights, \n",
    "    Materials, \n",
    "    RasterizationSettings, \n",
    "    MeshRenderer, \n",
    "    MeshRasterizer,  \n",
    "    SoftPhongShader,\n",
    "    SoftSilhouetteShader,\n",
    "    SoftPhongShader,\n",
    "    TexturesVertex,\n",
    "    AmbientLights\n",
    ")\n",
    "\n",
    "# add path for demo utils functions \n",
    "import sys\n",
    "import os\n",
    "sys.path.append(os.path.abspath(''))\n",
    "\n",
    "import wandb\n",
    "import pickle\n",
    "import random"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "8145c43f-1bc4-413c-afdf-9b8f0ebbd94b",
   "metadata": {},
   "outputs": [],
   "source": [
    "if torch.cuda.is_available():\n",
    "    device = torch.device(\"cuda:0\")\n",
    "    torch.cuda.set_device(device)\n",
    "else:\n",
    "    device = torch.device(\"cpu\")\n",
    "\n",
    "num_views = 20\n",
    "\n",
    "\n",
    "elev = torch.linspace(0, 360, num_views)\n",
    "azim = torch.linspace(-180, 180, num_views)\n",
    "\n",
    "\n",
    "lights = PointLights(device=device, location=[[0.0, 0.0, -3.0]])\n",
    "\n",
    "\n",
    "R, T = look_at_view_transform(dist=2.7, elev=elev, azim=azim)\n",
    "\n",
    "cameras = OpenGLPerspectiveCameras(device=device, R=R, T=T)\n",
    "\n",
    "camera = OpenGLPerspectiveCameras(device=device, R=R[None, 1, ...], \n",
    "                                  T=T[None, 1, ...]) \n",
    "\n",
    "sigma = 1e-4\n",
    "raster_settings_soft = RasterizationSettings(\n",
    "    image_size=64, \n",
    "    blur_radius=np.log(1. / 1e-4 - 1.)*sigma, \n",
    "    faces_per_pixel=50, \n",
    "    perspective_correct=False,\n",
    "    max_faces_per_bin=500000\n",
    ")\n",
    "\n",
    "\n",
    "# Differentiable soft renderer using per vertex RGB colors for texture\n",
    "renderer_textured = MeshRenderer(\n",
    "    rasterizer=MeshRasterizer(\n",
    "        cameras=camera, \n",
    "        raster_settings=raster_settings_soft\n",
    "    ),\n",
    "    shader=SoftPhongShader(device=device, \n",
    "        cameras=camera,\n",
    "        lights=lights)\n",
    ")\n",
    "\n",
    "\n",
    "def visualize_texture(tex):\n",
    "    to_vis = torch.squeeze(tex)\n",
    "    plt.imshow(to_vis.detach().cpu().numpy())\n",
    "    plt.show()\n",
    "    \n",
    "    \n",
    "    \n",
    "def load_mesh_and_target_rgb(obj_filename):\n",
    "    # Load obj file\n",
    "    mesh = load_objs_as_meshes([obj_filename], device=device)\n",
    "    \n",
    "    \n",
    "    verts = mesh.verts_packed()\n",
    "    N = verts.shape[0]\n",
    "    center = verts.mean(0)\n",
    "    scale = max((verts - center).abs().max(0)[0])\n",
    "    mesh.offset_verts_(-center)\n",
    "    mesh.scale_verts_((1.0 / float(scale)))\n",
    "    \n",
    "    meshes = mesh.extend(num_views)\n",
    "\n",
    "    target_images = renderer_textured(meshes, cameras=cameras, lights=lights)\n",
    "\n",
    "    target_rgb = [target_images[i, ..., :3] for i in range(num_views)]\n",
    "    target_cameras = [OpenGLPerspectiveCameras(device=device, R=R[None, i, ...], \n",
    "                                           T=T[None, i, ...]) for i in range(num_views)]\n",
    "\n",
    "\n",
    "    return mesh, target_images, target_cameras\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "16c85779-5488-4a21-a71e-3c7bc3171974",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Dataset3D:\n",
    "    def __init__(self, dataset_directory, number_of_examples, number_of_views, invalid_data_file=None, fakes=True):\n",
    "        self.dataset_directory = dataset_directory\n",
    "        self.number_of_examples = number_of_examples\n",
    "        self.number_of_views = number_of_views\n",
    "        self.invalid_data_file = invalid_data_file\n",
    "        self.invalid_data = []\n",
    "        self.data_names = []\n",
    "        self.current_index = 0\n",
    "        self.load_data()\n",
    "        self.fake_textures = None\n",
    "        self.fakes=fakes\n",
    "    \n",
    "    def load_data(self):\n",
    "        if self.invalid_data_file is not None:\n",
    "            with open(self.invalid_data_file, \"rb\") as input_file:\n",
    "                self.invalid_data = pickle.load(input_file)\n",
    "\n",
    "        for dire in os.listdir(self.dataset_directory):\n",
    "            obj_filename = os.path.join(self.dataset_directory, dire, 'normalized_model.obj')\n",
    "            if obj_filename in self.invalid_data:\n",
    "                continue\n",
    "            if dire[0] == '.':\n",
    "                continue\n",
    "            self.data_names.append(obj_filename)\n",
    "        self.data_names = sorted(self.data_names)\n",
    "        \n",
    "    def give_fake_textures(self, textures):\n",
    "        self.fake_textures = textures\n",
    "        self.fake_textures = torch.permute(self.fake_textures, (0, 2,3,1))\n",
    "            \n",
    "    def load_mesh_and_target_rgb(self, obj_filename):\n",
    "        \n",
    "        mesh = load_objs_as_meshes([obj_filename], device=device)\n",
    "        verts = mesh.verts_packed()\n",
    "        N = verts.shape[0]\n",
    "        center = verts.mean(0)\n",
    "        scale = max((verts - center).abs().max(0)[0])\n",
    "        mesh.offset_verts_(-center)\n",
    "        mesh.scale_verts_((1.0 / float(scale)))\n",
    "        meshes = mesh.extend(num_views)\n",
    "\n",
    "        target_images = renderer_textured(meshes, cameras=cameras, lights=lights)\n",
    "\n",
    "        target_rgb = [target_images[i, ..., :3] for i in range(num_views)]\n",
    "        target_cameras = [OpenGLPerspectiveCameras(device=device, R=R[None, i, ...], \n",
    "                                               T=T[None, i, ...]) for i in range(num_views)]\n",
    "\n",
    "\n",
    "        return mesh, target_rgb, target_cameras\n",
    "    \n",
    "    \n",
    "    def __iter__(self):\n",
    "        return self\n",
    "\n",
    "    def __next__(self): # Python 2: def next(self)\n",
    "        end_index = self.current_index + self.number_of_examples\n",
    "        start_index = self.current_index\n",
    "        if end_index >= (len(self.data_names)):\n",
    "            end_index = len(self.data_names)\n",
    "        if start_index >= end_index:\n",
    "            self.current_index = 0\n",
    "            raise StopIteration\n",
    "        self.current_index += self.number_of_examples\n",
    "        current_data_batch = self.data_names[start_index:end_index]\n",
    "        \n",
    "        real_data = []\n",
    "        for batch_elem in current_data_batch:\n",
    "            mesh, target_rgb, _ = self.load_mesh_and_target_rgb(batch_elem)\n",
    "            indices_to_keep = random.sample(range(20), self.number_of_views)\n",
    "            target_rgb = [target_rgb[i] for i in indices_to_keep]\n",
    "            real_data.extend(target_rgb)\n",
    "            \n",
    "        for k in range(len(real_data)):\n",
    "            real_data[k] = torch.unsqueeze(real_data[k],0)\n",
    "        \n",
    "        real_data = torch.cat(real_data, 0)\n",
    "        real_data = torch.permute(real_data, (0, 3, 1, 2))\n",
    "        \n",
    "        print(type(real_data))\n",
    "            \n",
    "        if not self.fakes:\n",
    "            return real_data\n",
    "        \n",
    "        if not self.fake_textures:\n",
    "            print('No texture given.')\n",
    "            raise Exception\n",
    "    \n",
    "        fake_data = []\n",
    "        for j,batch_elem in enumerate(current_data_batch):\n",
    "            mesh, target_rgb, _ = self.load_mesh_and_target_rgb(batch_elem)\n",
    "            mesh.textures._maps_padded = torch.squeeze(fake_perm[j], 0)\n",
    "                    \n",
    "            mesh = mesh.extend(num_views)\n",
    "        \n",
    "            fake_predicted = renderer_textured(mesh, cameras=cameras, lights=lights)\n",
    "            fake_rgb = [fake_predicted[i, ..., :3] for i in range(num_views)]\n",
    "            \n",
    "            \n",
    "            \n",
    "            indices_to_keep = random.sample(range(20), self.number_of_views)\n",
    "            fake_rgb = [fake_rgb[i] for i in indices_to_keep]\n",
    "            fake_data.extend(target_rgb) \n",
    "        \n",
    "        for k in range(len(fake_data)):\n",
    "            fake_data[k] = torch.unsqueeze(fake_data[k],0)\n",
    "        \n",
    "        fake_data = torch.cat(fake_data, 0)\n",
    "        fake_data = torch.permute(fake_data, (0, 3, 1, 2))\n",
    "        \n",
    "        return real_data, fake_data\n",
    "        \n",
    "    \n",
    "            \n",
    "    \n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "id": "e577313c-cc3a-41e9-b285-30327f58c870",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'torch.Tensor'>\n",
      "torch.Size([48, 3, 64, 64])\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'torch.Tensor'>\n",
      "<class 'torch.Tensor'>\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Input \u001b[0;32mIn [59]\u001b[0m, in \u001b[0;36m<cell line: 3>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m dataset \u001b[38;5;241m=\u001b[39m Dataset3D( \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m/projects/3DDatasets/3D-FUTURE/3D-FUTURE-model\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;241m3\u001b[39m, \u001b[38;5;241m16\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mnerenderovatelne.txt\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;28;01mFalse\u001b[39;00m)\n\u001b[0;32m----> 3\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m i,data \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28menumerate\u001b[39m(dataset):\n\u001b[1;32m      4\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m i \u001b[38;5;241m%\u001b[39m \u001b[38;5;241m10\u001b[39m \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m0\u001b[39m:\n\u001b[1;32m      5\u001b[0m         \u001b[38;5;28mprint\u001b[39m(data\u001b[38;5;241m.\u001b[39msize())\n",
      "Input \u001b[0;32mIn [53]\u001b[0m, in \u001b[0;36mDataset3D.__next__\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m     67\u001b[0m real_data \u001b[38;5;241m=\u001b[39m []\n\u001b[1;32m     68\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m batch_elem \u001b[38;5;129;01min\u001b[39;00m current_data_batch:\n\u001b[0;32m---> 69\u001b[0m     mesh, target_rgb, _ \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mload_mesh_and_target_rgb\u001b[49m\u001b[43m(\u001b[49m\u001b[43mbatch_elem\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     70\u001b[0m     indices_to_keep \u001b[38;5;241m=\u001b[39m random\u001b[38;5;241m.\u001b[39msample(\u001b[38;5;28mrange\u001b[39m(\u001b[38;5;241m20\u001b[39m), \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mnumber_of_views)\n\u001b[1;32m     71\u001b[0m     target_rgb \u001b[38;5;241m=\u001b[39m [target_rgb[i] \u001b[38;5;28;01mfor\u001b[39;00m i \u001b[38;5;129;01min\u001b[39;00m indices_to_keep]\n",
      "Input \u001b[0;32mIn [53]\u001b[0m, in \u001b[0;36mDataset3D.load_mesh_and_target_rgb\u001b[0;34m(self, obj_filename)\u001b[0m\n\u001b[1;32m     40\u001b[0m mesh\u001b[38;5;241m.\u001b[39mscale_verts_((\u001b[38;5;241m1.0\u001b[39m \u001b[38;5;241m/\u001b[39m \u001b[38;5;28mfloat\u001b[39m(scale)))\n\u001b[1;32m     41\u001b[0m meshes \u001b[38;5;241m=\u001b[39m mesh\u001b[38;5;241m.\u001b[39mextend(num_views)\n\u001b[0;32m---> 43\u001b[0m target_images \u001b[38;5;241m=\u001b[39m \u001b[43mrenderer_textured\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmeshes\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcameras\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcameras\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mlights\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mlights\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     45\u001b[0m target_rgb \u001b[38;5;241m=\u001b[39m [target_images[i, \u001b[38;5;241m.\u001b[39m\u001b[38;5;241m.\u001b[39m\u001b[38;5;241m.\u001b[39m, :\u001b[38;5;241m3\u001b[39m] \u001b[38;5;28;01mfor\u001b[39;00m i \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(num_views)]\n\u001b[1;32m     46\u001b[0m target_cameras \u001b[38;5;241m=\u001b[39m [OpenGLPerspectiveCameras(device\u001b[38;5;241m=\u001b[39mdevice, R\u001b[38;5;241m=\u001b[39mR[\u001b[38;5;28;01mNone\u001b[39;00m, i, \u001b[38;5;241m.\u001b[39m\u001b[38;5;241m.\u001b[39m\u001b[38;5;241m.\u001b[39m], \n\u001b[1;32m     47\u001b[0m                                        T\u001b[38;5;241m=\u001b[39mT[\u001b[38;5;28;01mNone\u001b[39;00m, i, \u001b[38;5;241m.\u001b[39m\u001b[38;5;241m.\u001b[39m\u001b[38;5;241m.\u001b[39m]) \u001b[38;5;28;01mfor\u001b[39;00m i \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(num_views)]\n",
      "File \u001b[0;32m~/miniconda3/envs/envx/lib/python3.9/site-packages/torch/nn/modules/module.py:1110\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m   1106\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1107\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1108\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1109\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1110\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1111\u001b[0m \u001b[38;5;66;03m# Do not call functions when jit is used\u001b[39;00m\n\u001b[1;32m   1112\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[38;5;241m=\u001b[39m [], []\n",
      "File \u001b[0;32m~/miniconda3/envs/envx/lib/python3.9/site-packages/pytorch3d/renderer/mesh/renderer.py:60\u001b[0m, in \u001b[0;36mMeshRenderer.forward\u001b[0;34m(self, meshes_world, **kwargs)\u001b[0m\n\u001b[1;32m     47\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m     48\u001b[0m \u001b[38;5;124;03mRender a batch of images from a batch of meshes by rasterizing and then\u001b[39;00m\n\u001b[1;32m     49\u001b[0m \u001b[38;5;124;03mshading.\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m     57\u001b[0m \u001b[38;5;124;03mFor this set rasterizer.raster_settings.clip_barycentric_coords=True\u001b[39;00m\n\u001b[1;32m     58\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m     59\u001b[0m fragments \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mrasterizer(meshes_world, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m---> 60\u001b[0m images \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mshader\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfragments\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmeshes_world\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     62\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m images\n",
      "File \u001b[0;32m~/miniconda3/envs/envx/lib/python3.9/site-packages/torch/nn/modules/module.py:1110\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m   1106\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1107\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1108\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1109\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1110\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1111\u001b[0m \u001b[38;5;66;03m# Do not call functions when jit is used\u001b[39;00m\n\u001b[1;32m   1112\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[38;5;241m=\u001b[39m [], []\n",
      "File \u001b[0;32m~/miniconda3/envs/envx/lib/python3.9/site-packages/pytorch3d/renderer/mesh/shader.py:125\u001b[0m, in \u001b[0;36mSoftPhongShader.forward\u001b[0;34m(self, fragments, meshes, **kwargs)\u001b[0m\n\u001b[1;32m    123\u001b[0m materials \u001b[38;5;241m=\u001b[39m kwargs\u001b[38;5;241m.\u001b[39mget(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mmaterials\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmaterials)\n\u001b[1;32m    124\u001b[0m blend_params \u001b[38;5;241m=\u001b[39m kwargs\u001b[38;5;241m.\u001b[39mget(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mblend_params\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mblend_params)\n\u001b[0;32m--> 125\u001b[0m colors \u001b[38;5;241m=\u001b[39m \u001b[43mphong_shading\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    126\u001b[0m \u001b[43m    \u001b[49m\u001b[43mmeshes\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmeshes\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    127\u001b[0m \u001b[43m    \u001b[49m\u001b[43mfragments\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mfragments\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    128\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtexels\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtexels\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    129\u001b[0m \u001b[43m    \u001b[49m\u001b[43mlights\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mlights\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    130\u001b[0m \u001b[43m    \u001b[49m\u001b[43mcameras\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcameras\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    131\u001b[0m \u001b[43m    \u001b[49m\u001b[43mmaterials\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmaterials\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    132\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    133\u001b[0m znear \u001b[38;5;241m=\u001b[39m kwargs\u001b[38;5;241m.\u001b[39mget(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mznear\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;28mgetattr\u001b[39m(cameras, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mznear\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;241m1.0\u001b[39m))\n\u001b[1;32m    134\u001b[0m zfar \u001b[38;5;241m=\u001b[39m kwargs\u001b[38;5;241m.\u001b[39mget(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mzfar\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;28mgetattr\u001b[39m(cameras, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mzfar\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;241m100.0\u001b[39m))\n",
      "File \u001b[0;32m~/miniconda3/envs/envx/lib/python3.9/site-packages/pytorch3d/renderer/mesh/shading.py:81\u001b[0m, in \u001b[0;36mphong_shading\u001b[0;34m(meshes, fragments, lights, cameras, materials, texels)\u001b[0m\n\u001b[1;32m     79\u001b[0m verts \u001b[38;5;241m=\u001b[39m meshes\u001b[38;5;241m.\u001b[39mverts_packed()  \u001b[38;5;66;03m# (V, 3)\u001b[39;00m\n\u001b[1;32m     80\u001b[0m faces \u001b[38;5;241m=\u001b[39m meshes\u001b[38;5;241m.\u001b[39mfaces_packed()  \u001b[38;5;66;03m# (F, 3)\u001b[39;00m\n\u001b[0;32m---> 81\u001b[0m vertex_normals \u001b[38;5;241m=\u001b[39m \u001b[43mmeshes\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mverts_normals_packed\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m  \u001b[38;5;66;03m# (V, 3)\u001b[39;00m\n\u001b[1;32m     82\u001b[0m faces_verts \u001b[38;5;241m=\u001b[39m verts[faces]\n\u001b[1;32m     83\u001b[0m faces_normals \u001b[38;5;241m=\u001b[39m vertex_normals[faces]\n",
      "File \u001b[0;32m~/miniconda3/envs/envx/lib/python3.9/site-packages/pytorch3d/structures/meshes.py:757\u001b[0m, in \u001b[0;36mMeshes.verts_normals_packed\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    750\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mverts_normals_packed\u001b[39m(\u001b[38;5;28mself\u001b[39m):\n\u001b[1;32m    751\u001b[0m     \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m    752\u001b[0m \u001b[38;5;124;03m    Get the packed representation of the vertex normals.\u001b[39;00m\n\u001b[1;32m    753\u001b[0m \n\u001b[1;32m    754\u001b[0m \u001b[38;5;124;03m    Returns:\u001b[39;00m\n\u001b[1;32m    755\u001b[0m \u001b[38;5;124;03m        tensor of normals of shape (sum(V_n), 3).\u001b[39;00m\n\u001b[1;32m    756\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[0;32m--> 757\u001b[0m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_compute_vertex_normals\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    758\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_verts_normals_packed\n",
      "File \u001b[0;32m~/miniconda3/envs/envx/lib/python3.9/site-packages/pytorch3d/structures/meshes.py:881\u001b[0m, in \u001b[0;36mMeshes._compute_vertex_normals\u001b[0;34m(self, refresh)\u001b[0m\n\u001b[1;32m    878\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (refresh \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28many\u001b[39m(v \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;28;01mfor\u001b[39;00m v \u001b[38;5;129;01min\u001b[39;00m [\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_verts_normals_packed])):\n\u001b[1;32m    879\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m\n\u001b[0;32m--> 881\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39misempty():\n\u001b[1;32m    882\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_verts_normals_packed \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mzeros(\n\u001b[1;32m    883\u001b[0m         (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_N, \u001b[38;5;241m3\u001b[39m), dtype\u001b[38;5;241m=\u001b[39mtorch\u001b[38;5;241m.\u001b[39mint64, device\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdevice\n\u001b[1;32m    884\u001b[0m     )\n\u001b[1;32m    885\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "dataset = Dataset3D( \"/projects/3DDatasets/3D-FUTURE/3D-FUTURE-model\", 3, 16, 'nerenderovatelne.txt', False)\n",
    "\n",
    "for i,data in enumerate(dataset):\n",
    "    if i % 10 == 0:\n",
    "        print(data.size())\n",
    "        texture = torch.permute(data[0], (1,2,0))\n",
    "        visualize_texture(texture)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fda6e94e-0253-4b68-a4ec-b0b3652c1067",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
